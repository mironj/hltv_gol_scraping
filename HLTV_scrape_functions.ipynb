{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f975815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba86250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(url):\n",
    "    \"\"\"Returns parsed HTML using ScrapingBee API\"\"\"\n",
    "    \n",
    "    response = requests.get(\n",
    "        url='https://app.scrapingbee.com/api/v1/',\n",
    "        params={\n",
    "            'api_key': 'your-api-key',\n",
    "            'url': url,\n",
    "            'block_resources':'False'\n",
    "        }, \n",
    "    )\n",
    "    return(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6968d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cs_team_id_name (year):\n",
    "    \"\"\"Returns two lists:\n",
    "    1. List of team HLTV IDs (strings) from the given year's top 85 teams (based on Rating 2.0)\n",
    "    2. List of team corresponding names in format used in HLTV URLs\"\"\"\n",
    "    \n",
    "    url = f'https://www.hltv.org/stats/teams?startDate={year}-01-01&endDate={year}-12-31'\n",
    "    \n",
    "    page = send_request(url)\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    elements = soup.find_all('td', class_=\"teamCol-teams-overview\")\n",
    "    ID_name = []\n",
    "    for element in elements:\n",
    "        line = str(element)\n",
    "        ID_name.append(line)\n",
    "    pattern = re.compile(r\"/stats/teams/(.*?)\\?startDate\")\n",
    "    ID_name = [', '.join(pattern.findall(s)) for s in ID_name]\n",
    "    \n",
    "    ID = []\n",
    "    name = []\n",
    "    for item in ID_name:\n",
    "        parts = item.split('/')\n",
    "        ID.append(parts[0])\n",
    "        name.append(parts[1])\n",
    "        \n",
    "    return ID, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edd5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cs_team_info (team_name, team_id, year):\n",
    "    \"\"\"Returns a dataframe containing info on all lineups of a given team in a given year:\n",
    "    1. End and start date of each lineup\n",
    "    2. Maps played by each lineup\n",
    "    3. Wins, losses, draws of each lineup\n",
    "    4. Names, maps and ratings played by each player in each lineup\n",
    "    5. Country of origin of each player in each lineup\n",
    "    (Maps and rating of each player is a culminative value from entire year on this team, not on a particular lineup)\n",
    "    (Moreover, one-time substitutions are ommited, thus the total number of games for a particular player can be higher than the sum of games played by all lineups)\"\"\"\n",
    "    \n",
    "    url = f'https://www.hltv.org/stats/teams/lineups/{team_id}/{team_name}?startDate={year}-01-01&endDate={year}-12-31'\n",
    "    url1 = f'https://www.hltv.org/stats/teams/players/{team_id}/{team_name}?startDate={year}-01-01&endDate={year}-12-31'\n",
    "    \n",
    "    page = send_request(url)\n",
    "    page1 = send_request(url1)\n",
    "    \n",
    "    if page.status_code == 200 and page1.status_code == 200:\n",
    "    \n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        elements = soup.find_all(\"div\", class_=\"lineup-container\")\n",
    "        lines = []\n",
    "        for element in elements:\n",
    "            element_lines = element.get_text(\"\\n\", strip=True).split(\"\\n\")\n",
    "            lines.extend([line for line in element_lines if line.strip()])\n",
    "        all_content = \"\\n\".join(lines)\n",
    "\n",
    "        lines = all_content.split('\\n')\n",
    "\n",
    "        start_dates = lines[::25]\n",
    "        end_dates = lines[2::25]\n",
    "\n",
    "        maps = lines[19::25]\n",
    "\n",
    "        records = lines[21::25]\n",
    "        wins = [int(record.split('/')[0].strip()) for record in records]\n",
    "        draws = [int(record.split('/')[1].strip()) for record in records]\n",
    "        losses = [int(record.split('/')[2].strip()) for record in records]\n",
    "\n",
    "        first_players = lines[4::25]\n",
    "        second_players =  lines[7::25]\n",
    "        third_players =  lines[10::25]\n",
    "        fourth_players =  lines[13::25]\n",
    "        fifth_players =  lines[16::25]\n",
    "\n",
    "        start_dates = {\"Start date\": start_dates}\n",
    "        end_dates = {\"End date\": end_dates}\n",
    "        maps = {\"Maps\": maps}\n",
    "        wins = {\"Wins\": wins}\n",
    "        draws = {\"Draws\": draws}\n",
    "        losses = {\"Losses\": losses}\n",
    "        first_players = {\"Player 1\": first_players}\n",
    "        second_players = {\"Player 2\": second_players}\n",
    "        third_players = {\"Player 3\": third_players}\n",
    "        fourth_players = {\"Player 4\": fourth_players}\n",
    "        fifth_players = {\"Player 5\": fifth_players}\n",
    "\n",
    "        dicts_df = [\n",
    "            start_dates,\n",
    "            end_dates,\n",
    "            maps,\n",
    "            wins,\n",
    "            draws,\n",
    "            losses,\n",
    "            first_players,\n",
    "            second_players,\n",
    "            third_players,\n",
    "            fourth_players,\n",
    "            fifth_players\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            combined_dict_df = {}\n",
    "            for d in dicts_df:\n",
    "                combined_dict_df.update(d)\n",
    "\n",
    "            df = pd.DataFrame(combined_dict_df).transpose()\n",
    "            new_rows = ['Player 1 maps', 'Player 2 maps', 'Player 3 maps', 'Player 4 maps', 'Player 5 maps', \n",
    "                        'Player 1 rating', 'Player 2 rating', 'Player 3 rating', 'Player 4 rating', 'Player 5 rating', \n",
    "                        'Player 1 country', 'Player 2 country', 'Player 3 country', 'Player 4 country', 'Player 5 country']\n",
    "            for row_label in new_rows:\n",
    "                df.loc[row_label] = pd.Series()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(page1.content, \"html.parser\")\n",
    "        elements = soup.find_all(\"tbody\")\n",
    "        lines = []\n",
    "        for element in elements:\n",
    "            element_lines = element.get_text(\"\\n\", strip=True).split(\"\\n\")\n",
    "            lines.extend([line for line in element_lines if line.strip()])\n",
    "        extra_content = \"\\n\".join(lines)\n",
    "\n",
    "        elements = soup.find_all(\"td\")\n",
    "        elements = [td.find_all('img') for td in elements]\n",
    "        elements = [img_tag for sublist in elements for img_tag in sublist]\n",
    "        country = [img.get('alt') for img in elements]\n",
    "\n",
    "        lines = extra_content.split('\\n')\n",
    "        names = lines[::6]\n",
    "        names = [string for string in names if not string.isdigit()]\n",
    "        n_players = len(names)\n",
    "        maps = lines[1::6]\n",
    "        maps = maps[:n_players]\n",
    "        maps = {\"Maps\": maps}\n",
    "        rating = lines[5::6]\n",
    "        rating = rating[:n_players]\n",
    "        rating = {\"Rating\": rating}\n",
    "        country = {\"Country\": country}\n",
    "\n",
    "        dicts_extra = [\n",
    "            maps,\n",
    "            rating,\n",
    "            country\n",
    "        ]\n",
    "        try:\n",
    "            combined_dict_extra = {}\n",
    "            for d in dicts_extra:\n",
    "                combined_dict_extra.update(d)\n",
    "\n",
    "            extra = pd.DataFrame(combined_dict_extra).transpose()\n",
    "            extra.columns = names\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        for col in df.columns:\n",
    "            for n in range(5):\n",
    "                name = df[col][n+6]\n",
    "                df[col][f\"Player {n+1} maps\"] = extra[name][\"Maps\"]\n",
    "                df[col][f\"Player {n+1} rating\"] = extra[name][\"Rating\"]\n",
    "                df[col][f\"Player {n+1} country\"] = extra[name][\"Country\"]\n",
    "\n",
    "        for col in range(df.shape[1]):\n",
    "            columns = df.columns.tolist()\n",
    "            columns[col] = f'{team_name}_{year}_{col+1}'\n",
    "            df.columns = columns\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping pipeline - creates a concatenated dataframe of top 85 teams from all given years and saves it to .csv.\n",
    "#The number of times scraping API has to be used is smaller or equal (if there are no repeating teams in the given years) to 86n, where n is the number of given years.\n",
    "#Unused free version of Scraping Bee API provides 200 uses of API.\n",
    "\n",
    "years = []\n",
    "for year in range(2018, 2023):\n",
    "    years.append(str(year))\n",
    "    \n",
    "all_IDs = []\n",
    "all_names = []\n",
    "    \n",
    "for year in years:\n",
    "    IDs, names = get_cs_team_id_name(year)\n",
    "    for ID, name in zip(IDs, name):\n",
    "        if ID not in all_IDs:\n",
    "            all_IDs.append(ID)\n",
    "            all_names.append(name)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for year in years:\n",
    "    for ID, name in zip(all_IDs, all_names):\n",
    "        df = get_cs_team_info(name, ID, year)\n",
    "        data = pd.concat([data, df], axis=1, sort=False)  \n",
    "        \n",
    "data.to_csv('cs_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
